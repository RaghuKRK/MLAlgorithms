#Linear Regression By Gradient Descent
#Gradient Descent is the process of minimizing a function by following the gradients of the cost function.
 #y= B0 +B1 * x
setwd("C:/Users/Desktop/Raghu/R")
getwd()

###Load the Data
data <- read.table("ex1data1.txt",sep= ",")
data
str(data)
A <- as.data.frame(rep(1,nrow(data[1])))
X <- data[1]
X <- as.data.frame(cbind(A,X))
X
colnames(X)[colnames(X)=="rep(1, nrow(data[1]))"] <- "A"
str(X)
X <- as.matrix(X)
y <- data[2]
m <- nrow(data)

#Compute Cost Function 

  cost_function =  function(X,y,m,theta)
  {
    theta=as.matrix(theta,1,2)
    J = (1/(2*m)) * sum(((X %*% theta) - y)^2)
    print(J)
  }
  cost_function(X,y,m,theta=c(0.0))
  cost_function(X,y,m,theta=c(-1,2))


# Gradient Descent
  gradient_descent_function = function (X,y,theta,alpha,iteration)
  {
    m= nrow(X)
    J_history = rep(0,iteration)
    theta=as.matrix(theta,1,2)
    for(i in 1:iteration)
    {
      theta_0 = theta[1] - (alpha / m * sum((X %*% theta) - y))
      theta_1 = theta[2] - (alpha / m * sum((X %*% theta) - y) * X[2])
      theta = as.matrix(c(theta_0,theta_1),1,2)
      print(theta)
      ##print(i)
      J_history = cost_function(X,y,m,theta)   
    }
  }
  gradient_descent_function(X,y,theta=c(0,0),0.1,50)
